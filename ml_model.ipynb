{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdfa276e6f4f219",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:11:48.015331Z",
     "start_time": "2024-04-19T08:11:46.117784Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868e947474149ca0",
   "metadata": {},
   "source": [
    "# Annotation and Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d65caa545990e3d",
   "metadata": {},
   "source": [
    "Step 1: Parsing the Phenotype File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619c029974705ca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:11:51.623015Z",
     "start_time": "2024-04-19T08:11:49.986392Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_phenotype_file(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter='\\t', encoding='windows-1252', header=None)\n",
    "    df.columns = ['Patient_ID', 'Disease', 'Position']\n",
    "    # Store both Disease and Position in a tuple\n",
    "    phenotype_dict = {row['Patient_ID']: (row['Disease'], row['Position']) for index, row in df.iterrows()}\n",
    "    return phenotype_dict\n",
    "\n",
    "phenotype_file_path = '/mnt/sdb/phenotype-pseudon.txt'\n",
    "phenotype_data = parse_phenotype_file(phenotype_file_path)\n",
    "print(phenotype_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e672e6ebbe3096d",
   "metadata": {},
   "source": [
    "Me loeme sisse kõik VCFid sõltumata nende eksisteerimisest globals filest ning annoteerime need ära lisades globals filest leitud phenotype data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df57180bdfa4b91f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:11:53.597590Z",
     "start_time": "2024-04-19T08:11:53.572830Z"
    }
   },
   "outputs": [],
   "source": [
    "def export_to_pandas_and_scale(mt, destination, vcf_filename, phenotype_info):\n",
    "    disease, position = phenotype_info\n",
    "    group = classify_disease(disease, position)\n",
    "    \n",
    "    csv_destination = os.path.join(destination, group + '-csv')  # Append '-csv' to the group folder\n",
    "\n",
    "    if not os.path.exists(csv_destination):\n",
    "        os.makedirs(csv_destination)\n",
    "    save_path = os.path.join(csv_destination, vcf_filename.replace('.vcf', '.csv'))\n",
    "    \n",
    "    mt = mt.drop('vep') # Drop the 'vep' struct to simplify the DataFrame export\n",
    "    df = mt.rows().flatten().to_pandas()    # Convert to pandas DataFrame\n",
    "    \n",
    "    # Add 'phenotype_position' from 'phenotype_info'\n",
    "    df['phenotype_position'] = position  # Use the unpacked position\n",
    "\n",
    "    # Mapping and Encoding\n",
    "    impact_mapping = {'HIGH': 0, 'MODERATE': 1, 'LOW': 2, 'MODIFIER': 3}\n",
    "    df['IMPACT'] = df['IMPACT'].map(impact_mapping)\n",
    "\n",
    "    # Ordinal Encoding for gene symbols and gene IDs\n",
    "    encoder = OrdinalEncoder()\n",
    "    df[['SYMBOL', 'HGNC_ID']] = encoder.fit_transform(df[['SYMBOL', 'HGNC_ID']])\n",
    "\n",
    "    # Scaling numeric values\n",
    "    scaler = MinMaxScaler()\n",
    "    df['MAX_AF'] = scaler.fit_transform(df[['MAX_AF']])\n",
    "\n",
    "    # Selecting and rearranging the final DataFrame columns\n",
    "    final_columns = ['IMPACT', 'SYMBOL', 'HGNC_ID', 'MAX_AF', 'phenotype_position'] \n",
    "    df_final = df[final_columns]\n",
    "\n",
    "    df_final.to_csv(save_path, index=False) # Save the DataFrame to a CSV file\n",
    "    print(f\"Exported {save_path}\")\n",
    "    return save_path  # Ensure this path is returned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0384a1013f764ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:11:55.176585Z",
     "start_time": "2024-04-19T08:11:55.169919Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_disease(disease, position):\n",
    "    if disease == 'TERVE':\n",
    "        return 'positive-group-terve'\n",
    "    elif disease == 'RV' and position != 'NA':\n",
    "        return 'positive-group-RV'\n",
    "    else:\n",
    "        return 'negative-group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d0d630506e28e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:17:56.105738Z",
     "start_time": "2024-04-19T08:11:57.228192Z"
    }
   },
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "\n",
    "def vcfs_to_matrixtable(source, phenotype_data, vep_config_path, base_destination, write=True, export_to_pandas=True, log_file='processed_vcfs_log.txt'):\n",
    "    files = []\n",
    "        \n",
    "    if os.path.isdir(source):\n",
    "        files = [os.path.join(source, f) for f in os.listdir(source) if f.endswith('.vcf') or f.endswith('.vcf.gz')]\n",
    "    elif os.path.isfile(source) and (source.endswith('.vcf') or source.endswith('.vcf.gz')):\n",
    "        files.append(source)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid path or file type. Must be a directory or a VCF file.\")\n",
    "\n",
    "    assert files, \"No VCF files found at the specified location.\"\n",
    "\n",
    "    hl.init(default_reference='GRCh37')  # Initialize Hail\n",
    "\n",
    "    # Contig recoding for import_vcf\n",
    "    contig_recoding = {f\"chr{i}\": str(i) for i in range(1, 23)}\n",
    "    contig_recoding.update({\"chrX\": \"X\", \"chrY\": \"Y\"})\n",
    "    log_entries = []\n",
    "    \n",
    "    processed_vcfs = [] #TOOD: \n",
    "\n",
    "    # Import and annotate files in a loop\n",
    "    try:\n",
    "        for vcf in files:\n",
    "        \n",
    "            patient_code = os.path.basename(vcf).split('_')[0]\n",
    "            phenotype_info = phenotype_data.get(patient_code, (\"NA\", \"NA\"))\n",
    "            \n",
    "            disease, position = phenotype_info\n",
    "            group = classify_disease(disease, position)\n",
    "            \n",
    "            destination = os.path.join(base_destination, group)\n",
    "            destination_path = os.path.join(destination, os.path.basename(vcf).replace('.vcf', '.mt'))\n",
    "     \n",
    "            if os.path.exists(destination_path):\n",
    "                print(f\"Skipping {vcf}, as the output file already exists in {destination_path}.\")\n",
    "                continue\n",
    "                \n",
    "            if not os.path.exists(destination):\n",
    "                os.makedirs(destination)\n",
    "    \n",
    "    \n",
    "            mt = hl.import_vcf(vcf, force_bgz=True, reference_genome='GRCh37', contig_recoding=contig_recoding, skip_invalid_loci=True)\n",
    "            mt = hl.vep(mt, config=vep_config_path)\n",
    "            \n",
    "            \n",
    "            print(\"VEP output structure:\")\n",
    "            mt.vep.describe()\n",
    "            \n",
    "            mt = mt.annotate_globals(phenotype_disease=phenotype_info[0], phenotype_position=phenotype_info[1])\n",
    "            mt = mt.filter_rows(mt.alleles[1] != \"*\") # filter star alleles \n",
    "            mt = mt.annotate_rows(\n",
    "                IMPACT=mt.vep.IMPACT, # HIGH/MODERATE\n",
    "                SYMBOL=mt.vep.SYMBOL, # Categorical\n",
    "                HGNC_ID=mt.vep.HGNC_ID, # Categorical\n",
    "                MAX_AF=mt.vep.MAX_AF, # Numeric field, allele frequency\n",
    "                MAX_AF_POPS=mt.vep.MAX_AF_POPS # Populations with max allele frequency\n",
    "            )\n",
    "                    \n",
    "            if write:\n",
    "                destination_path = os.path.join(destination, os.path.basename(vcf).replace('.vcf', '.mt'))\n",
    "                mt.write(destination_path)\n",
    "                \n",
    "            csv_path = export_to_pandas_and_scale(mt, destination, os.path.basename(vcf), phenotype_info) if export_to_pandas else 'Not Exported'\n",
    "            log_entry = f\"{vcf}, {phenotype_info[0]}, {phenotype_info[1]}, {destination_path}, {csv_path}\"\n",
    "\n",
    "            log_entries.append(log_entry)\n",
    "    \n",
    "                \n",
    "            if export_to_pandas:\n",
    "                export_to_pandas_and_scale(mt, destination, os.path.basename(vcf), phenotype_info)\n",
    "                processed_vcfs.append({'vcf': vcf, 'mt_path': destination_path})\n",
    "            \n",
    "\n",
    "    finally:         \n",
    "        hl.stop()  # Stop Hail context when done\n",
    "        with open(log_file, 'w') as file:\n",
    "            file.write('\\n'.join(log_entries))\n",
    "    \n",
    "    return processed_vcfs, log_entries\n",
    "\n",
    "# Example usage:\n",
    "VEP_CONFIG_PATH = '/home/markus/gen-toolbox/src/config/vep_settings.json'\n",
    "SOURCE_DIR = '/mnt/sdb/TSHC_data_VCF/E01381784_S4.annotated2.vcf'\n",
    "base_destination_directory = '/home/markus/gen-toolbox/output'\n",
    "phenotype_data = parse_phenotype_file('/mnt/sdb/phenotype-pseudon.txt') \n",
    "log_entries = vcfs_to_matrixtable(SOURCE_DIR, phenotype_data, VEP_CONFIG_PATH, base_destination_directory, write=True, export_to_pandas=True)\n",
    "\n",
    "print(log_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165cac298f5ede93",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27828f9a2097bc02",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e14b4697f72414c",
   "metadata": {},
   "source": [
    "## Prepare Datasets for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae50a5805680c6",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "def load_data(base_dir, group_name):\n",
    "    # Use glob to handle multiple CSV files\n",
    "    csv_dir = os.path.join(base_dir, group_name, group_name + '-csv')\n",
    "    files = glob(os.path.join(csv_dir, \"*.csv\"))\n",
    "    data_list = [pd.read_csv(file) for file in files]\n",
    "    if data_list:\n",
    "        data = pd.concat(data_list)\n",
    "        data['group'] = group_name  # Add a column indicating the group\n",
    "    else:\n",
    "        data = pd.DataFrame()\n",
    "    return data\n",
    "\n",
    "def load_all_groups(base_directory):\n",
    "    groups = [\"positive-group-terve\", \"positive-group-RV\", \"negative-group\"]\n",
    "    data_frames = {}\n",
    "    for group in groups:\n",
    "        data_frames[group] = load_data(base_directory, group) # Loading data for each group\n",
    "        data_frames[group]['label'] = group  # Label the data based on group name\n",
    "    print(data_frames, \"Data frames loaded successfully.\")\n",
    "    return data_frames\n",
    "\n",
    "def prepare_datasets(data_frames):\n",
    "    # Sampling for validation set\n",
    "    print(\"Starting to prepare datasets...\")\n",
    "    data_terve_val = data_frames[\"positive-group-terve\"].sample(frac=0.66, random_state=42)\n",
    "    data_rv_val = data_frames[\"positive-group-RV\"].sample(frac=0.33, random_state=42)\n",
    "    validation_data = pd.concat([data_terve_val, data_rv_val])\n",
    "\n",
    "    # Remaining data\n",
    "    data_terve_remain = data_frames[\"positive-group-terve\"].drop(data_terve_val.index)\n",
    "    data_rv_remain = data_frames[\"positive-group-RV\"].drop(data_rv_val.index)\n",
    "    remaining_data = pd.concat([data_terve_remain, data_rv_remain, data_frames[\"negative-group\"]])\n",
    "\n",
    "    # Split remaining data into training and testing\n",
    "    training_data, testing_data = train_test_split(remaining_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    return training_data, validation_data, testing_data\n",
    "\n",
    "base_directory = '/home/markus/gen-toolbox/output'\n",
    "data_frames = load_all_groups(base_directory)\n",
    "training_data, validation_data, testing_data = prepare_datasets(data_frames)\n",
    "\n",
    "# Optionally, save the datasets to files\n",
    "training_data.to_csv(f\"{base_directory}/training_set.csv\", index=False)\n",
    "validation_data.to_csv(f\"{base_directory}/validation_set.csv\", index=False)\n",
    "testing_data.to_csv(f\"{base_directory}/testing_set.csv\", index=False)\n",
    "\n",
    "# TODO: Load the datasets according to the previous dir paths - csv files are placed under /home/markus/gen-toolbox/output/positive-group-RV/positive-group-RV-csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3278be5dfb252ce",
   "metadata": {},
   "source": [
    "## Machine Learning Models\n",
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d2cf4455c8b86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:28:06.403388Z",
     "start_time": "2024-04-15T08:28:06.397259Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "params = {\n",
    "    'max_depth': 3,  # Maximum depth of a tree\n",
    "    'objective': 'binary:logistic',  # Objective function (multiclass classification)\n",
    "    'eta': 0.3,  # Learning rate \n",
    "    'eval_metric': 'logloss',\n",
    "    'random_state': 42\n",
    "}\n",
    "num_rounds = 100\n",
    "\n",
    "# Creating DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(training_data.drop(['label', 'group'], axis=1), label=training_data['label'].map({'positive-group-terve': 1, 'positive-group-RV': 1, 'negative-group': 0}))\n",
    "dval = xgb.DMatrix(validation_data.drop(['label', 'group'], axis=1), label=validation_data['label'].map({'positive-group-terve': 1, 'positive-group-RV': 1, 'negative-group': 0}))\n",
    "dtest = xgb.DMatrix(testing_data.drop(['label', 'group'], axis=1), label=testing_data['label'].map({'positive-group-terve': 1, 'positive-group-RV': 1, 'negative-group': 0}))\n",
    "\n",
    "# Training the model\n",
    "model = xgb.train(params, dtrain, num_rounds, evals=[(dtrain, 'train'), (dval, 'validation')])\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(dtest)\n",
    "# Threshold of 0.5: If the predicted probability is >0.5, classify as 1 (positive); otherwise, 0 (negative).\n",
    "predictions = [1 if p >= 0.5 else 0 for p in predictions]\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(testing_data['label'].map({'positive-group-terve': 1, 'positive-group-RV': 1, 'negative-group': 0}), predictions)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3d0083cfeeb29",
   "metadata": {},
   "source": [
    "## TabPFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af5f605c2f5ce7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T19:08:06.138774Z",
     "start_time": "2024-04-18T19:08:05.847180Z"
    }
   },
   "outputs": [],
   "source": [
    "from tabpfn import TabPFNClassifier\n",
    "import time \n",
    "\n",
    "start = time.time()\n",
    "# Initialize the TabPFN classifier\n",
    "classifier = TabPFNClassifier(device='cpu', N_ensemble_configurations=32) # 32 different models will be trained\n",
    "\n",
    "# Fit the model\n",
    "classifier.fit(training_data.drop('labels', axis=1).values, training_data['labels'].values)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = classifier.predict(testing_data.drop('labels', axis=1).values)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(testing_data['labels'].values, predictions)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# TODO: Calculate power\n",
    "# TODO: Calculate precision\n",
    "# TODO: calculate sensitivity\n",
    "# TODO: calculate model accuracy\n",
    "# TODO: calculate perfomance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4bf3fc97c00344",
   "metadata": {},
   "source": [
    "# TODO: end usecase, load in single VCF using our trained model and predict the phenotype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f7e0a067f2b861",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
